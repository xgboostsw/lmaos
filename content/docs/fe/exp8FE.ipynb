{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cd-1hqTieVw",
        "outputId": "c92b4115-6654-4212-d94d-c0a9d891e13d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['blue', 'bright', 'can', 'in', 'is', 'see', 'shining', 'sky', 'sun', 'the', 'we']\n",
            "\n",
            "Bag of Words Matrix:\n",
            "\n",
            "   blue  bright  can  in  is  see  shining  sky  sun  the  we\n",
            "0     1       0    0   0   1    0        0    1    0    1   0\n",
            "1     0       1    0   0   1    0        0    0    1    1   0\n",
            "2     0       1    0   1   1    0        0    1    1    2   0\n",
            "3     0       1    1   0   0    1        1    0    2    2   1\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    \"The sky is blue.\",\n",
        "    \"The sun is bright.\",\n",
        "    \"The sun in the sky is bright.\",\n",
        "    \"We can see the shining sun, the bright sun.\"\n",
        "]\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "tokenized_corpus = [preprocess(doc) for doc in corpus]\n",
        "\n",
        "all_tokens = [token for doc in tokenized_corpus for token in doc]\n",
        "\n",
        "vocab = sorted(set(all_tokens))\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "def vectorize(doc_tokens, vocab):\n",
        "    word_count = Counter(doc_tokens)\n",
        "    return [word_count[word] for word in vocab]\n",
        "\n",
        "bow_vectors = [vectorize(doc, vocab) for doc in tokenized_corpus]\n",
        "\n",
        "df_bow = pd.DataFrame(bow_vectors, columns=vocab)\n",
        "print(\"\\nBag of Words Matrix:\\n\")\n",
        "print(df_bow)\n"
      ]
    }
  ]
}