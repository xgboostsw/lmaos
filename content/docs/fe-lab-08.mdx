---
title: FE Lab 08 - Bag of Words (BoW) Implementation
description: Another implementation of Bag of Words text vectorization
---

## Aim
To reinforce understanding of Bag of Words text vectorization through alternative implementation and practice.

## Theory

### Bag of Words - Recap
A fundamental text representation technique in Natural Language Processing (NLP) that:
- Converts text into numerical vectors
- Based on word frequency
- Ignores word order and grammar
- Creates sparse, high-dimensional vectors

### Mathematical Representation

For a corpus with vocabulary V and document d:

```
BoW(d) = [f₁, f₂, ..., f|V|]
```

Where fᵢ is the frequency of word i in document d.

### Example Calculation

```
Document: "The sun in the sky is bright"
Vocabulary: [bright, in, is, sky, sun, the]

Word frequencies:
- bright: 1
- in: 1
- is: 1
- sky: 1
- sun: 1
- the: 2

BoW Vector: [1, 1, 1, 1, 1, 2]
```

### Important Considerations

1. **Preprocessing Impact**:
   - Different preprocessing → Different vectors
   - Consistency is crucial
   - Standard pipeline: lowercase → remove punctuation → tokenize

2. **Vocabulary Management**:
   - Vocabulary size grows with corpus size
   - Can limit to top-k most frequent words
   - Can set minimum/maximum document frequency

3. **Sparsity Problem**:
   - Large vocabulary → Sparse vectors
   - Most values are zero
   - Memory inefficient
   - Solution: Use sparse matrix representations

### Comparison with Other Methods

| Method | Word Order | Semantics | Dimensionality |
|--------|-----------|-----------|----------------|
| BoW | ❌ Lost | ❌ No | High |
| TF-IDF | ❌ Lost | ❌ No | High |
| Word2Vec | ✅ Partial | ✅ Yes | Fixed (low) |
| BERT | ✅ Yes | ✅ Yes | Fixed (high) |

## Algorithm

1. Define a corpus of text documents
2. Create preprocessing function:
   - Convert text to lowercase
   - Remove all punctuation characters
   - Split into tokens (words)
3. Apply preprocessing to all documents
4. Build vocabulary:
   - Extract all unique tokens
   - Sort alphabetically
5. Define vectorization function:
   - Count occurrences of each vocabulary word
   - Return frequency vector
6. Apply vectorization to all documents
7. Create Bag of Words matrix (DataFrame)
8. Display vocabulary and matrix

## Program

```python
import string
from collections import Counter
import pandas as pd

corpus = [
    "The sky is blue.",
    "The sun is bright.",
    "The sun in the sky is bright.",
    "We can see the shining sun, the bright sun."
]

def preprocess(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = text.split()
    return tokens

tokenized_corpus = [preprocess(doc) for doc in corpus]

all_tokens = [token for doc in tokenized_corpus for token in doc]

vocab = sorted(set(all_tokens))
print("Vocabulary:", vocab)

def vectorize(doc_tokens, vocab):
    word_count = Counter(doc_tokens)
    return [word_count[word] for word in vocab]

bow_vectors = [vectorize(doc, vocab) for doc in tokenized_corpus]

df_bow = pd.DataFrame(bow_vectors, columns=vocab)
print("\nBag of Words Matrix:\n")
print(df_bow)
```

## Output
The program displays:
1. **Vocabulary List**: All unique words in sorted order
2. **Bag of Words Matrix**: Document-term frequency matrix

Expected output structure:
```
Vocabulary: ['blue', 'bright', 'can', 'in', 'is', 'see', 'shining', 'sky', 'sun', 'the', 'we']

Bag of Words Matrix:
   blue  bright  can  in  is  see  shining  sky  sun  the  we
0     1       0    0   0   1    0        0    1    0    2   0
1     0       1    0   0   1    0        0    0    1    2   0
2     0       1    0   1   1    0        0    1    1    3   0
3     0       1    1   0   0    1        1    0    2    3   1
```

## Observations

1. **Implementation Flexibility**:
   - Same result as Lab 07
   - Multiple ways to implement BoW
   - Python's Counter class simplifies counting

2. **Word Frequency Patterns**:
   - "the" appears most frequently (stop word)
   - Content words (sun, sky, bright) carry meaning
   - Frequency ≠ Importance

3. **Document Representation**:
   - Each document becomes a point in n-dimensional space
   - n = vocabulary size
   - Similar documents have similar vectors

4. **Scalability Issues**:
   - 4 documents → 11 features
   - 1000 documents → potentially 10,000+ features
   - Need dimensionality reduction techniques

### Practical Applications

1. **Spam Classification**:
   - Train classifier on BoW features
   - Words like "free", "win", "click" indicate spam
   - Simple yet effective

2. **Sentiment Analysis**:
   - Positive words → Positive sentiment
   - Negative words → Negative sentiment
   - Combined with labeled training data

3. **Document Clustering**:
   - Group similar documents
   - Based on word overlap
   - K-means or hierarchical clustering

## Result
Successfully implemented an alternative Bag of Words vectorization approach. Reinforced understanding of:
- Text preprocessing pipeline
- Vocabulary construction
- Frequency-based vectorization
- Matrix representation of text data

Key takeaways:
- BoW is simple but effective
- Foundation for advanced NLP techniques
- Trade-off between simplicity and information loss
- Preprocessing choices significantly impact results
- Ready for use with ML algorithms
