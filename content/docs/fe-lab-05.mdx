---
title: FE Lab 05 - Standardization Impact on Convergence
description: Comparing Logistic Regression performance with and without standardization
---

## Aim
To analyze the impact of standardization on Logistic Regression convergence speed and accuracy using the Breast Cancer dataset.

## Theory

### Standardization (Z-score Normalization)
Transforms features to have mean = 0 and standard deviation = 1.

**Formula**:
$$
X_{standardized} = \frac{X - \mu}{\sigma}
$$

Where:
- μ (mu) = mean of the feature
- σ (sigma) = standard deviation of the feature

### Standardization vs Min-Max Scaling

| Aspect | Standardization | Min-Max Scaling |
|--------|----------------|-----------------|
| Range | Unbounded (-∞ to +∞) | Bounded [0, 1] |
| Mean | 0 | Varies |
| Std Dev | 1 | Varies |
| Outliers | Less sensitive | More sensitive |
| Use Case | Gaussian distribution assumed | Known min/max bounds |

### Why Standardization Matters for Convergence

1. **Gradient Descent Optimization**:
   - Unscaled features create elongated, elliptical cost surfaces
   - Gradient descent takes zigzag path
   - More iterations needed to reach minimum
   
2. **With Standardization**:
   - Cost surface becomes more circular/spherical
   - Gradient descent takes direct path to minimum
   - Faster convergence with fewer iterations

3. **Numerical Stability**:
   - Prevents overflow/underflow in calculations
   - Improves precision in matrix operations

### Convergence
The process by which an optimization algorithm approaches the optimal solution. Measured by:
- Number of iterations
- Time taken
- Final accuracy achieved

## Algorithm

1. Load Breast Cancer dataset
2. Split features (X) and target (y)
3. Split into train (70%) and test (30%) sets with stratification
4. **Without Standardization**:
   - Train Logistic Regression (max_iter=5000, solver='lbfgs')
   - Record number of iterations until convergence
   - Calculate accuracy on test set
5. **With Standardization**:
   - Fit StandardScaler on training data
   - Transform both train and test data
   - Train Logistic Regression (same parameters)
   - Record number of iterations until convergence
   - Calculate accuracy on test set
6. Compare:
   - Accuracy improvement
   - Iteration count difference
7. Display all metrics

## Program

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

logreg_no_scaling = LogisticRegression(max_iter=5000, solver="lbfgs")
logreg_no_scaling.fit(X_train, y_train)

y_pred_no_scaling = logreg_no_scaling.predict(X_test)
acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)
iterations_no_scaling = logreg_no_scaling.n_iter_
print("Logistic Regression without Standardization:")
print(f"Accuracy: {acc_no_scaling:.4f}")
print(f"Iterations until convergence: {iterations_no_scaling}")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logreg_scaled = LogisticRegression(max_iter=5000, solver="lbfgs")
logreg_scaled.fit(X_train_scaled, y_train)

y_pred_scaled = logreg_scaled.predict(X_test_scaled)
acc_scaled = accuracy_score(y_test, y_pred_scaled)
iterations_scaled = logreg_scaled.n_iter_
print("\nLogistic Regression with Standardization:")
print(f"Accuracy: {acc_scaled:.4f}")
print(f"Iterations until convergence: {iterations_scaled}")

print("\nPerformance Comparison:")
print(f"Accuracy improvement: {acc_scaled - acc_no_scaling:.4f}")
print(f"Iteration difference: {int(iterations_no_scaling - iterations_scaled)}")
```

## Output
The program displays:
1. **Without Standardization**:
   - Accuracy: ~0.9415
   - Iterations until convergence: ~340

2. **With Standardization**:
   - Accuracy: ~0.9825
   - Iterations until convergence: ~11

3. **Performance Comparison**:
   - Accuracy improvement: ~0.041 (4.1%)
   - Iteration difference: ~329 iterations saved

## Observations

1. **Convergence Speed**:
   - Standardization reduces iterations by ~97%
   - Significantly faster training time
   - More efficient optimization

2. **Model Accuracy**:
   - 4.1% improvement with standardization
   - Better feature weight distribution
   - More reliable predictions

3. **Practical Implications**:
   - Essential for large datasets (faster training)
   - Reduces computational cost
   - Prevents convergence warnings
   - Improves model stability

4. **Why Such Dramatic Improvement**:
   - Breast Cancer features have vastly different scales
   - Some features: 0-1 range
   - Other features: 0-1000+ range
   - Standardization levels the playing field

## Result
Successfully demonstrated the critical impact of standardization on Logistic Regression convergence. Standardization reduced iterations by 97% (from ~340 to ~11) while improving accuracy by 4.1%. This proves that standardization is essential for:
- Faster model training
- Better convergence
- Improved accuracy
- Efficient gradient descent optimization
