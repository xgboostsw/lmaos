---
title: Lab 04 - Multiple Layer Perceptron Forward Pass
description: Forward pass for a small multi-layer perceptron with several activation functions
---

## Installation

Install the required dependencies:

```bash
pip install numpy
```

# Multiple Layer Perceptron: Forward Pass

This lab demonstrates a forward pass through a small multilayer perceptron and shows results using various activation functions.

```python
# Exp4 - Multiple Layer Perceptron Forward Pass

import numpy as np
np.set_printoptions(precision=4)

# Weights for Layer 1 and Layer 2
W_0 = np.array([[-1, -1, 1],
                [-1,  2, 1]], dtype=float)
print(f'{W_0.shape=}')

W_1 = np.array([[1, -1, 2]], dtype=float)
print(f'{W_1.shape=}')

# Input
X = np.array([[0, -1, 1],
              [-1, 0, -1]], dtype=float)
print(f'{X.shape=}')

# Activation functions
def USigmoid(x):
    return 1 / (1 + np.exp(-x))

def BSigmoid(x):
    return (1 - np.exp(-x)) / (1 + np.exp(-x))

def ReLU(x):
    return np.array([max(i, 0) for i in x])

def Lin(x):
    return x

def activation(Z, fcn='Lin'):
    A = np.array(list(map(globals()[fcn], Z)))
    return A

# Forward Pass
f_1 = 'USigmoid'
f_2 = 'USigmoid'

if X.shape[0] == W_0.shape[1]:
    A_0 = X
else:
    A_0 = np.vstack((np.ones((1, X.shape[1])), X))
print(f'{A_0 =}')

Z_1 = W_0 @ A_0
A_1 = activation(Z_1, f_1)
print(f'{Z_1 = }')
print(f'{A_1 = }')

if A_1.shape[0] + 1 == W_1.shape[1]:
    A_1 = np.vstack((np.ones((1, A_1.shape[1])), A_1))
print(f'{A_1 = }')

Z_2 = W_1 @ A_1
A_2 = activation(Z_2, f_2)
print(f'{Z_2 = }')
print(f'{A_2 = }')
```

## Notes
- This demonstrates a forward pass through two layers with configurable activation functions.
- The bias is added to the activation output before passing to the next layer.
- You can experiment with different activation function combinations by changing `f_1` and `f_2`.
