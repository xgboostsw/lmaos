---
title: FE Lab 04 - Min-Max Scaling Impact
description: Analyzing the impact of Min-Max scaling on Logistic Regression and KNN
---

## Aim
To analyze and compare the impact of Min-Max scaling on the performance of Logistic Regression and K-Nearest Neighbors (KNN) classifiers using the Breast Cancer dataset.

## Theory

### Feature Scaling
Feature scaling is a preprocessing technique that standardizes the range of independent variables. It's crucial for algorithms that compute distances or use gradient descent.

### Min-Max Scaling (Normalization)
Transforms features to a fixed range, typically [0, 1].

**Formula**:

```
X_scaled = (X - X_min) / (X_max - X_min)
```

**Properties**:
- Preserves the shape of original distribution
- Bounded between 0 and 1
- Sensitive to outliers
- All features contribute equally to distance calculations

### Why Scaling Matters

1. **Distance-Based Algorithms** (KNN, K-Means):
   - Features with larger scales dominate distance calculations
   - Example: Age (0-100) vs. Income (0-100,000)
   - Without scaling, income would dominate

2. **Gradient Descent Optimization** (Logistic Regression, Neural Networks):
   - Unscaled features cause irregular cost function surfaces
   - Slower convergence
   - Risk of getting stuck in local minima

3. **Regularization**:
   - L1/L2 penalties treat all features equally
   - Unscaled features receive unfair penalties

### Algorithms Used

**K-Nearest Neighbors (KNN)**:
- Classifies based on k closest training examples
- Uses Euclidean distance (highly scale-sensitive)
- Default k=5

**Logistic Regression**:
- Linear classifier using sigmoid function
- Optimized via gradient descent
- Convergence speed depends on feature scales

## Algorithm

1. Load Breast Cancer dataset (30 features, binary target)
2. Split features (X) and target (y)
3. Split into train (70%) and test (30%) sets
4. **Without Scaling**:
   - Train Logistic Regression (max_iter=200)
   - Predict and calculate accuracy
   - Train KNN classifier
   - Predict and calculate accuracy
5. **With Min-Max Scaling**:
   - Fit scaler on training data
   - Transform both train and test data
   - Train Logistic Regression (max_iter=500)
   - Predict and calculate accuracy
   - Train KNN classifier
   - Predict and calculate accuracy
6. Visualize results using bar charts for both models
7. Display all accuracy metrics

## Program

```python
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

# Load the dataset
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target  # 0 = malignant, 1 = benign

# Split the data into features (X) and target (y)
X = df.drop('target', axis=1)
y = df['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# --- Logistic Regression ---

# Without scaling
model_unscaled = LogisticRegression(max_iter=200)
model_unscaled.fit(X_train, y_train)
y_pred_unscaled = model_unscaled.predict(X_test)
acc_unscaled = accuracy_score(y_test, y_pred_unscaled)
print(" * Logistic Regression Accuracy without Min-Max scaling:", acc_unscaled)

# With scaling
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model_scaled = LogisticRegression(max_iter=500)
model_scaled.fit(X_train_scaled, y_train)
y_pred_scaled = model_scaled.predict(X_test_scaled)
acc_scaled = accuracy_score(y_test, y_pred_scaled)
print(" * Logistic Regression Accuracy after Min-Max scaling:", acc_scaled)


# --- KNN ---

# Without scaling
knn_unscaled = KNeighborsClassifier()
knn_unscaled.fit(X_train, y_train)
y_pred_knn_unscaled = knn_unscaled.predict(X_test)
acc_knn_unscaled = accuracy_score(y_test, y_pred_knn_unscaled)
print(" * KNN Accuracy without scaling:", acc_knn_unscaled)

# With scaling
knn_scaled = KNeighborsClassifier()
knn_scaled.fit(X_train_scaled, y_train)
y_pred_knn_scaled = knn_scaled.predict(X_test_scaled)
acc_knn_scaled = accuracy_score(y_test, y_pred_knn_scaled)
print(" * KNN Accuracy with scaling:", acc_knn_scaled)


# --- Visualization ---

# Bar plot for Logistic Regression
labels = ['Before Scaling', 'After Scaling']
accuracies = [acc_unscaled, acc_scaled]
plt.figure(figsize=(6, 5))
bars = plt.bar(labels, accuracies, color=['tomato', 'seagreen'])
# Annotate bars with accuracy values
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.005,
             f'{yval:.4f}', ha='center', va='bottom', fontsize=12)
plt.ylim(0.9, 1.0)  # adjust based on your accuracy range
plt.title('Logistic Regression Accuracy: Before vs After Min-Max Scaling')
plt.ylabel('Accuracy')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


# Bar plot for KNN
labels = ['Before Scaling', 'After Scaling']
accuracies = [acc_knn_unscaled, acc_knn_scaled]
plt.figure(figsize=(6, 5))
bars = plt.bar(labels, accuracies, color=['tomato', 'seagreen'])
# Annotate bars with accuracy values
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.005,
             f'{yval:.4f}', ha='center', va='bottom', fontsize=12)
plt.ylim(0.9, 1.0)  # adjust based on your accuracy range
plt.title('KNN Accuracy: Before vs After Min-Max Scaling')
plt.ylabel('Accuracy')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```

## Output
The program generates:
1. **Accuracy Metrics**:
   - Logistic Regression without scaling: ~0.92
   - Logistic Regression with scaling: ~0.98
   - KNN without scaling: ~0.92
   - KNN with scaling: ~0.97

2. **Bar Charts**: 
   - Visual comparison before and after scaling for both models
   - Annotated with exact accuracy values

## Observations

1. **KNN Performance**:
   - Dramatic improvement with scaling (~5% increase)
   - Distance calculations become more meaningful
   - All features contribute fairly to classification

2. **Logistic Regression Performance**:
   - Significant improvement with scaling (~6% increase)
   - Faster convergence
   - Better gradient descent optimization

3. **Key Insights**:
   - Min-Max scaling is essential for distance-based algorithms
   - Scaling improves convergence in optimization algorithms
   - Always scale when features have different units/ranges
   - Fit scaler only on training data to avoid data leakage

## Result
Successfully demonstrated the critical impact of Min-Max scaling on machine learning models. Both Logistic Regression and KNN showed substantial accuracy improvements (5-6%) after scaling. This highlights the importance of feature scaling as a preprocessing step, especially for distance-based and gradient descent algorithms.
